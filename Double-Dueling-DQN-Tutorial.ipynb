{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Reinforcement Learning with Tensorflow Part 4: Deep Q-Networks andÂ Beyond\n",
    "\n",
    "In this iPython notebook I implement a Deep Q-Network using both Double DQN and Dueling DQN. The agent learn to solve a navigation task in a basic grid world. To learn more, read here: https://medium.com/p/8438a3e2b8df\n",
    "\n",
    "For more reinforcment learning tutorials, see:\n",
    "https://github.com/awjuliani/DeepRL-Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the game environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to adjust the size of the gridworld. Making it smaller provides an easier task for our DQN agent, while making the world larger increases the challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class gameOb():\n",
    "    def __init__(self,x,y,size,intensity,channel):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.size = size\n",
    "        self.intensity = intensity\n",
    "        self.channel = channel\n",
    "        \n",
    "class gameEnv():\n",
    "    def __init__(self):\n",
    "        self.sizeX = 6\n",
    "        self.sizeY = 6\n",
    "        self.actions = 4\n",
    "        a = self.reset()\n",
    "        plt.imshow(a,interpolation=\"nearest\")\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        xList = range(self.sizeX)\n",
    "        yList = range(self.sizeY)\n",
    "        Xs = np.random.choice(xList,replace=False,size=3)\n",
    "        Ys = np.random.choice(yList,replace=False,size=3)\n",
    "        hero = gameOb(Ys[0],Xs[0],1,1,2)\n",
    "        bug = gameOb(Ys[1],Xs[1],1,1,1)\n",
    "        hole = gameOb(Ys[2],Xs[2],1,1,0)\n",
    "        state = self.renderEnv([bug,hero,hole])\n",
    "        self.objects = [bug,hero,hole]\n",
    "        self.state = state\n",
    "        return state\n",
    "\n",
    "    def moveChar(self,objects,direction):\n",
    "        # 0 - up, 1 - down, 2 - left, 3 - right\n",
    "        bug,hero,hole = objects[0],objects[1],objects[2]\n",
    "        if direction == 0 and hero.y >= 1:\n",
    "            hero.y -= 1\n",
    "        if direction == 1 and hero.y <= self.sizeY-2:\n",
    "            hero.y += 1\n",
    "        if direction == 2 and hero.x >= 1:\n",
    "            hero.x -= 1\n",
    "        if direction == 3 and hero.x <= self.sizeX-2:\n",
    "            hero.x += 1        \n",
    "        return [bug,hero,hole]\n",
    "\n",
    "    def checkGoal(self,objects):\n",
    "        bug,hero,hole = objects[0],objects[1],objects[2]\n",
    "        if hero.x == bug.x and hero.y == bug.y:\n",
    "            return 1.,True\n",
    "        elif hero.x == hole.x and hero.y == hole.y:\n",
    "            return -1.,True\n",
    "        else:\n",
    "            return 0,False\n",
    "\n",
    "    def renderEnv(self,objects):\n",
    "        a = np.zeros([self.sizeY,self.sizeX,3])\n",
    "        for item in objects:\n",
    "            a[item.y:item.y+item.size,item.x:item.x+item.size,item.channel] = item.intensity\n",
    "        b = scipy.misc.imresize(a[:,:,0],[84,84,1],interp='nearest')\n",
    "        c = scipy.misc.imresize(a[:,:,1],[84,84,1],interp='nearest')\n",
    "        d = scipy.misc.imresize(a[:,:,2],[84,84,1],interp='nearest')\n",
    "        a = np.stack([b,c,d],axis=2)\n",
    "        return a\n",
    "\n",
    "    def step(self,action):\n",
    "        objects = self.moveChar(self.objects,action)\n",
    "        state = self.renderEnv(objects)\n",
    "        reward,done = self.checkGoal(objects)\n",
    "        return state,reward,done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD/CAYAAADRymv0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8279e5d950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gameEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is an example of a starting environment in our simple game. The agent controls the blue square, and can move up, down, left, or right. The goal is to move to the green square (for +1 reward) and avoid the red square (for -1 reward). The position of the three blocks is randomized every episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the network itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self,h_size):\n",
    "        #The network recieves a frame from the game, flattened into an array.\n",
    "        #It then resizes it and processes it through four convolutional layers.\n",
    "        self.scalarInput =  tf.placeholder(shape=[None,21168],dtype=tf.float32)\n",
    "        self.imageIn = tf.reshape(self.scalarInput,shape=[-1,84,84,3])\n",
    "        self.conv1 = tf.contrib.layers.convolution2d( \\\n",
    "            inputs=self.imageIn,num_outputs=32,kernel_size=[8,8],stride=[4,4],padding='VALID', biases_initializer=None)\n",
    "        self.conv2 = tf.contrib.layers.convolution2d( \\\n",
    "            inputs=self.conv1,num_outputs=64,kernel_size=[4,4],stride=[2,2],padding='VALID', biases_initializer=None)\n",
    "        self.conv3 = tf.contrib.layers.convolution2d( \\\n",
    "            inputs=self.conv2,num_outputs=64,kernel_size=[3,3],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        self.conv4 = tf.contrib.layers.convolution2d( \\\n",
    "            inputs=self.conv3,num_outputs=512,kernel_size=[7,7],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        \n",
    "        #We take the output from the final convolutional layer and split it into separate advantage and value streams.\n",
    "        self.streamAC,self.streamVC = tf.split(3,2,self.conv4)\n",
    "        self.streamA = tf.contrib.layers.flatten(self.streamAC)\n",
    "        self.streamV = tf.contrib.layers.flatten(self.streamVC)\n",
    "        self.AW = tf.Variable(tf.random_normal([h_size/2,env.actions]))\n",
    "        self.VW = tf.Variable(tf.random_normal([h_size/2,1]))\n",
    "        self.Advantage = tf.matmul(self.streamA,self.AW)\n",
    "        self.Value = tf.matmul(self.streamV,self.VW)\n",
    "        \n",
    "        #Then combine them together to get our final Q-values.\n",
    "        self.Qout = self.Value + tf.sub(self.Advantage,tf.reduce_mean(self.Advantage,reduction_indices=1,keep_dims=True))\n",
    "        self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "        #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions,env.actions,dtype=tf.float32)\n",
    "        \n",
    "        self.Q = tf.reduce_sum(tf.mul(self.Qout, self.actions_onehot), reduction_indices=1)\n",
    "        \n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class allows us to store experies and sample then randomly to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self,size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer,size)),[size,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple function to resize our game frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def processState(states):\n",
    "    return np.reshape(states,[21168])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function allows us to update the parameters of our target network with those of the primary network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def updateTarget(tfVars,sess):\n",
    "    total_vars = len(tfVars)\n",
    "    for idx,var in enumerate(tfVars[0:total_vars/2]):\n",
    "        sess.run(tfVars[idx+total_vars/2].assign(var.eval()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function allows us to discount our rewards for a given episode. This approach is called the Monte-Carlo method, since we apply it to all the rewards in a given episode. We will be using it in order to provide a more robust reward signal to the DQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting all the training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32 #How many experiences to use for each training step.\n",
    "update_freq = 4 #How often to perform a training step.\n",
    "y = .99 #Discount factor on the target Q-values\n",
    "startE = 1 #Starting chance of random action\n",
    "endE = 0.1 #Final chance of random action\n",
    "anneling_steps = 50000. #How many steps of training to reduce startE to endE.\n",
    "num_episodes = 10000 #How many episodes of game environment to train network with.\n",
    "pre_train_steps = 50000 #How many steps of random actions before training begins.\n",
    "load_model = False #Whether to load a saved model.\n",
    "path = \"./dqn\" #The path to save our model to.\n",
    "h_size = 512 #The size of the final convolutional layer before splitting it into Advantage and Value streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork(h_size)\n",
    "targetQN = Qnetwork(h_size)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "#Set the rate of random action decrease. \n",
    "e = startE\n",
    "stepDrop = (startE - endE)/anneling_steps\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "#Make a path for our model to be saved in.\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if load_model == True:\n",
    "        print 'Loading Model...'\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    sess.run(init)\n",
    "    updateTarget(trainables,sess) #Set the target network to be equal to the primary network.\n",
    "    for i in range(num_episodes):\n",
    "        episodeBuffer = experience_buffer()\n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        s = processState(s)\n",
    "        d = False\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        #The Q-Network\n",
    "        while j < 200: #If the agent takes longer than 200 moves to reach either of the blocks, end the trial.\n",
    "            j+=1\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                a = np.random.randint(0,4)\n",
    "            else:\n",
    "                a = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:[s]})[0]\n",
    "            s1,r,d = env.step(a)\n",
    "            s1 = processState(s1)\n",
    "            total_steps += 1\n",
    "            episodeBuffer.add(np.reshape(np.array([s,a,r,s1,d]),[1,5])) #Save the experience to our episode buffer.\n",
    "            \n",
    "            if total_steps > pre_train_steps:\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "            \n",
    "                if total_steps % (update_freq*1000) == 0:\n",
    "                    print \"Target network updated.\"\n",
    "                    updateTarget(trainables,sess)\n",
    "                \n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    trainBatch = myBuffer.sample(batch_size) #Get a random batch of experiences.\n",
    "                    #Below we perform the Double-DQN update to the target Q-values\n",
    "                    Q1 = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                    Q2 = sess.run(targetQN.Qout,feed_dict={targetQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                    end_multiplier = -(trainBatch[:,4] - 1)\n",
    "                    doubleQ = Q2[range(batch_size),Q1]\n",
    "                    targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)\n",
    "                    #Update the network with our target values.\n",
    "                    _ = sess.run(mainQN.updateModel, \\\n",
    "                        feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,0]),mainQN.targetQ:targetQ, mainQN.actions:trainBatch[:,1]})\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            \n",
    "            if d == True:\n",
    "\n",
    "                break\n",
    "        \n",
    "        #Get all experiences from this episode and discount their rewards.\n",
    "        episodeRewards = np.array(episodeBuffer.buffer)[:,2]\n",
    "        discountRewards = discount_rewards(episodeRewards)\n",
    "        bufferArray = np.array(episodeBuffer.buffer)\n",
    "        bufferArray[:,2] = discountRewards\n",
    "        episodeBuffer.buffer = zip(bufferArray)\n",
    "        #Add the discounted experiences to our experience buffer.\n",
    "        myBuffer.add(episodeBuffer.buffer)\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "        #Periodically save the model. \n",
    "        if i % 1000 == 0:\n",
    "            saver.save(sess,path+'/model-'+str(i)+'.cptk')\n",
    "            print \"Saved Model\"\n",
    "        if len(rList) % 10 == 0:\n",
    "            print total_steps,np.mean(rList[-10:]), e\n",
    "    saver.save(sess,path+'/model-'+str(i)+'.cptk')\n",
    "print \"Percent of succesful episodes: \" + str(sum(rList)/num_episodes) + \"%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking network learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean reward over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rMat = np.resize(np.array(rList),[len(rList)/100,100])\n",
    "rMean = np.average(rMat,1)\n",
    "plt.plot(rMean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Episode length over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "jMat = np.resize(np.array(jList),[len(jList)/100,100])\n",
    "jMean = np.average(jMat,1)\n",
    "plt.plot(jMean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
